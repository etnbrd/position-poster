\section*{Abstract}

% Web applications must constantly evolve, both in features and in performance.
% At first, the need for features is predominant.
% The development team logically arrange the features in modules.
% By keeping modules lights and decoupled, they avoid getting overwhelmed by the growing complexity of the application.
% But the need for performance grows with the audience of the application.
% At some point, the parallelism of a cluster becomes necessary.
% The development team needs to organize the code differently.
% A particularly efficient solution is to slice the logic into stages to form a parallel pipeline \cite{Welsh2000}.

% Both the organizations in modules and in stages are carried manually over the source code.
% We believe the two organizations interfere with each other.
% The organization in stages freezes the organization in modules.
% It limits the evolution of the application, thus impacts its economic growth.
% To lift this limitation, we propose to automate the manual slicing in stages.
% By keeping only one manual organization, we remove the interferences.
% Developers could benefit from the efficiency of the pipeline solution to scale in performance; while keeping a coherent module organization to continue improving features.






The popularity of Javascript recently exploded.
Most implementations of Javascript present an event-loop design.
Node.js is an example.
We believe this design is more efficient to build web applications than the classical thread approach \cite{Lei2014}.
An event-loop uses a single-core execution, and a global memory.
It relieves the developer from the synchronization burden, but limits the implementation to a single machine.
When the desired throughput augments, latency increases.
Physics currently prevents us from building CPU with faster clocks.
Eventually, the only solution to keep a reasonable latency is to use multiple machines.
A particularly efficient design to leverage this parallelism is to slice an application into stages to form a parallel pipeline \cite{Welsh2000}.
However, slicing an application into stages is a costly operation when done manually.
Every stage must be balanced to fit on a machine so as to avoid bottlenecks leading to wasted resources.
Moreover, for important changes in the application, the slicing is reconsidered to keep the balance.

We argue that an event-loop is similar to a parallel pipeline executing on a single-core.
We propose to automatically isolate stages in the application logic.
The resulting application can be distributed over a network of machines.
We believe this work will later allow to automatically balance stages to maximize throughput with the available resources.

% Transition

An event loop invokes handlers to react on events.
Handlers end without return, they asynchronously trigger the next handlers.
The call stacks of two handlers are disjoints.
Thus, it is possible to parallelize their execution, as long as causality is preserved.
Every event handlers becomes a stage in the pipeline.

In a mono-thread execution model like Node.js, the memory is global.
But the atomic execution of handlers removes the need for synchronization and isolation \cite{Adya2002}.
The memory holds both the communications between the handlers, and their state.
To parallelize the handlers, we isolate the state of each handler to reproduce exclusivity; and we identify their communications to reproduce a one way flow of messages.

We built a first incomplete compiler as a proof of concept.
After successful results with custom applications, we are building a complete compiler to transform real applications.





% Real-time web applications need to be highly concurrent, and to scale over a large cluster.
% % This is achieved through parallelism.
% We identify two types of parallelism to meet such requirements.
% % Data parallelism and pipeline parallelism.
% Data parallelism replicates the whole logic to process each request in a thread \cite{Behren2003}.
% Pipeline parallelism slices logic into stages, each on an independent thread \cite{Ousterhout1996}.
% We believe the two designs are on two ends of a design spectrum; and each application requires a specific combination of the two designs.
% SEDA \cite{Welsh2000}, Storm and Spark are frameworks to design such combinations.

% The two types of parallelism reveals the duality of threads.
% % Logical threads and execution threads.
% Data parallelism uses a logical thread for each request.
% It defines an encapsulation for the logic of processing a request.
% Pipeline parallelism uses an execution thread for each core.
% It leverages parallelism.
% The motives to use logical threads and execution threads are opposite.
% We argue that it is a mistake to map the former onto the latter.

% Pipeline parallelism implies to manually slice an application for its deployment.
% Because it is a manual process, it implies to reduce logical threads to execution threads.
% We believe it limits the design and the evolution of a web application.
% Behren \textit{et. al.} state that \textit{it is a mistake to attempt high concurrency without help from the compiler} \cite{Behren2003}.
% Following this advice, we propose to automate this slicing process.
% We expect to improve design flexibility by allowing developers to dissociate logical from execution threads.
% % It allows developers to separate logical from execution threads for pipeline parallelism.
% For this automation, we resolved two main issues.
% Execution distribution and memory distribution.

% After decades of improvements, the event and the thread model are now almost alike for a single core \cite{Adya2002}.
% Still, the event model conserves a particularity.
% Event handlers end without return, they asynchronously trigger the next handler execution.
% % Event handlers end without return.
% % The next handler asynchronously continues the execution.
% % Their executions are independent. % as long as the causality is conserved.
% The call stacks of two handlers are disjoints.
% Thus, it is possible to distribute the execution on multiple machines, as long as the causality is preserved.

% Each distributed handler needs to access the shared memory.
% Web applications still heavily rely on single databases to store the whole state.
% With recent trends around in-memory caches, we believe it is now viable to bring closer the logic and its state.
% We choose to partition and distribute the memory.
% We detect the dependencies of each handlers through static analysis.
% The handlers communicate through a one way flow of messages to exchange the shared states.

% We built a first incomplete compiler as a proof of concept.
% After successful results with custom applications, we are now on the process of building a complete compiler to transform real applications.